# 통계학과 머신러닝
- 머신러닝의 주요 개념들은 통계학에서 온 것들이 많다.
- 통계학은 작은 표본 데이터만으로 전체를 신뢰도 몇 %로 예측해야하므로 데이터의 샘플링, 해석과 처리, 그것을 바탕으로한 전체 데이터를 이해할 수 있는 모델을 만드는 과정을 거친다.
  - 머신 러닝 역시 비슷하다.
  - 유한한 학습 데이터 분석 하고 유용한 특징 추출 후 새로운 데이터를 예측할 일반화 모델을 만들어야 하기 때문이다.

# Bagging
- Boostrap aggregating
- Boostrap : 원래 통계에서 사용되던 용어이며 예를 들어 전세계 남자들의 평균 키를 구하려면 모든 인구를 잴 수 없으므로 적절한 방법으로 표본(sample)을 추출하고 그 표본으로 부터 전세계 남성의 평균 키를 추정하는 것이다.
- Boostapping : 전체 모집단의 분포를 확실하게 알 수 없는 경우 표본을 취한 후 그 표본이 전체 집단을 대표한다는 가정하에 전체의 분포를 예측할 수 있게 할 때 사용한다.
  - 표본을 전체라고 생각하고 많은 횟수에 걸쳐 동일 개수의 샘플을 복원 추출 한 후 각 샘플에 대한 분포를 연구한다.
  - 그 후 전체 표본의 분포와 샘플들 간의 분포 관계를 통해 전체 집단의 분포를 유추하는 방식이 Boostrapping이다.
- Bagging : Boostrap을 사용해 머신러닝/통계의 정확도를 높이고 안정성을 얻는 방식이다.
  - 실제로 머신러닝에서 많이 쓰이는 방식이다.
  - model averaging을 통해 성능을 높인다.
  - Regression의 경우 평균(model averaging)을 취해 분산을 줄이는 효과를 얻는다.
  - Classification의 경우 투표 효과(voting)을 통해 가장 많은 결과가 나오는 것을 취하는 방식을 사용한다.

![image](https://user-images.githubusercontent.com/69780812/139029585-a0e92f0a-ce58-4c63-b24b-6cf2bc40d6d3.png)

- Bagging을 사용해 전체 데이터로부터 일부 샘플을 Boostrap sample을 취한다.
- 위 sample로 곡선 추정 시 회색 처럼 구불구불한 곡선이 나오고, 평균을 취하게 되면 빨간색 처럼 비교적 부드러운 곡선을 얻을 수 있게 된다.
- 연산 능력이 충분하다면, 여러 개의 모델에 대해 학습을 진행하고, 그 학습의 결과가 많이 모이는 것을 최종 결과로 선택할 수 있으며 이때 **Bagging(Ensemble)**을 사용할 수 있다.

## Bagging을 적용하면 안되는 경우
- 대부분 효과적이지만 적용하면 안되는 경우가 있다.
- 표본 데이터가 상당히 적은 경우
  - 표본이 전체를 잘 반영하지 못한다.
- 데이터 잡음이 많은 경우
  - Outlier가 추정을 크게 왜곡 시킬 가능성이 존재한다.
- 데이터에 의존성이 있는 경우
  - 기본적으로 Boostrapping은 데이터가 독립적인 경우를 가정한다.

## Bagging Boosting 차이
- 둘다 모델 결합, ensemble 방식이라는 관점에서 동일하다.

- Bagging은 모든 Boostrap이 서로 독립적인 관계를 갖는다.
- 하지만 Boosting은 순차적으로 처리가 되며 에러가 발생하면 그 에러의 Weighting을 올리므로 현재 Weak learner가 이전 Weak learner의 영향을 받는다.
- Boosting은 최종적으로 weighted vote를 하지만 Bagging은 단순 Vote를 한다.
- Bagging은 분산을 줄이는 것이 목적이지만, Boosting은 바이어스를 줄이는 것이 주 목적이다.
- 잡음이 없는 데이터에서는 Boosting이 Bagging보다 우수하다.
- Bagging은 Overfitting 문제를 해결할 수 있지만, Boosting은 Overfitting 문제로 부터 자유롭지 못하다.